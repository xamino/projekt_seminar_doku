\documentclass[a4paper]{article}

% Because it looks better:
\usepackage{a4wide}
% Take care of input things:
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
% Packages for pictures:
\usepackage{graphicx}
\usepackage{float}
\usepackage{listings}
\lstset{numbers=left, numberstyle=\tiny, numbersep=5pt
,frame=single, captionpos=b}
\lstset{language=Perl}
% Package for urls:
\usepackage{url}
\usepackage{hyperref}
\usepackage{amsmath}
\bibliographystyle{alpha}

% Now let us define a few shortcuts:
% Correct sign for C#:
\newcommand{\CS}{C\nolinebreak\hspace{-.05em}\raisebox{.6ex}{\scriptsize\bf \#\ }}

% --------------------------------------------------------------------------------------------!
%
% NOTE: For better usage of LaTeX with GIT, please write each sentence on an own line, separating paragraphs
%       by the usual double returns. That way GIT can track each sentence on its own instead of always
%       having to track a paragraph as one block.
%
% --------------------------------------------------------------------------------------------!

\begin{document}

\title{Weight Effects on Jumping Motion}
\author{Andreas Köll, Johannes Bohner, Burkhard Hoppenstedt, Tamino Hartmann}
\date{\today}

\maketitle

\begin{abstract}
Abstract here. TODO!
\end{abstract}
\newpage

\section{Introduction}

This paper represents a rework of \cite{origin}.
We analyzed what the authors did and tried the method on a somewhat different study.
Here we show the documentation to our work, our conclusions, and how our work compares to the original work.

\section{Overview of Decomposing Biological Motion}

This section will briefly highlight interesting and important aspects from the original paper on which our work was based on.
The general goal of the paper was to implement a system on the computer capable of interpreting biological motion.
This was done by recording the walking motion of men and women with a motion capturing system on a treadmill and then putting the data through an algorithm to distinguish between the gender based on the biological motion.

As the recorded data was not insignificant in its size, a principle component analysis was used to decrease the data volume.
The original authors found that the first four principal components offered sufficient coverage of the data with a value of 98\%.\footnote{TODO: Sollen wir hier auch noch die Grafik einfügen? Eigentlich unnötig...}
Just the standard poses and their principal components however do not represent movement: they must be animated over time.
This was done by mapping the values to a sinus function modified by its parameters.\footnote{TODO: Evtl. die Parameter aufzählen? Brauchen wir das?}
The data could then be represented in a single, 229 dimensional vector consisting of the three components: the standard poses, the principal components, and the parameters for the sinus function.

The data from all the walkers was then analyzed by a simple linear comparison algorithm.
The computer managed to correctly classify 90\% correctly in the best case (dependent on the various different approaches).
This was better than the control group's average.

Finally, the authors used the algorithm backwards to synthesize stylized walkers.
These can be changed interactively, allowing a quick comparison between the two distinct extremes between the genders and any step between.

\section{Our Work}

Now we'll take a look at the work we did.
Our intend was to recreate the original work from ground up with slightly changed parameters.
This meant that we had to do a few things differently, most notably the hardware used to record the motion data.

The hardware used restricted us to a more simple motion to capture – basically, it had to be 2D and without any 3D overlaps.
To allow for a higher yield of subjects, we refrained from trying to differentiate gender and instead choose to differentiate between doing the motion with and without weights.
Various experiments with the coverage of the used sensor narrowed the motion we would be using down to 2: jumping jack and rope skipping.

All software used was written by us, based on the algorithms outlined in the original paper.
Basically we created 2 software pieces: one for recording the motion from the sensor and one for working with the data (although that is more a workflow with many scripts than a complete software).

\subsection{Equipment}

For our recording we used the "Kinect for Windows"\texttrademark \cite{kinect}, connected to a computer running "Microsoft Windows 7"\textregistered \ via USB.
The Kinect is mounted on a tripod to allow correct positioning.
The weights were applied with a weight-vest with a weight of 5 kilograms.
For the alternate motion, skipping, we also provided a rope.
We used normal tape to mark positions on the floor so that all recordings were made with the same measurements.

\subsection{Recording the Data}

The first software allows us to record the skeleton input stream from the Kinect and save it to text files.
We developed the application for recording the input stream from the Kinect in \CS with "Microsoft Visual Studio 2012" utilizing the "Microsoft Kinect Developer Version".
The program is currently available as open source here \cite{csprogram} and can be seen in Fig \ref{fig:programm}.

\begin{figure}
	\centering
	\includegraphics[width=8cm]{programm.jpg}
	\caption{Screenshot of the program used to record the data.}
	\label{fig:programm}
\end{figure}

\subsection{Processing the Data}

The other software piece consisted of a multitude of MATLAB\cite{matlabprograms} scripts that were run in sequence on the files to generate our outputs. Ideally, this should be packaged into a single program too, but we considered the gain to be too small compared to the amount of work required for our project.
Our proposal for this is a node based program where one can manipulate the flow of the data through various external MATLAB scripts; the nodes would be the scripts and the connecting vectors the data moving between them\footnote{An example for the functionality can be found here: \url{http://www.blender.org/development/release-logs/blender-242/blender-composite-nodes/}}.
Such a program could be a future work with a wide array of alternate uses beyond this project.

As the overall workflow of the data from its capture to final results is non-trivial, we have provided a flowchart of the procedure in Fig \ref{fig:workflow}.
The chart should clear up the flow of data in our project from the recording to the various intermediate steps to our final results.

\begin{figure}
	\centering
	\includegraphics[width=14cm]{matlabaufbau.png}
	\caption{Overview of the workflow of the data.}
	\label{fig:workflow}
\end{figure}

\subsection{Composition of the Recording}

Figure \ref{fig:schematic} shows the schematics of the arrangement of the Kinect for the recordings.
The way the Kinect records skeletons proved to be a strong limiting factor in selecting which motions to record as the depth-perception is comparatively poor.
The fixed nature of the experiment also limited the motion to one that could be done within a small area, which disclosed for example any form of walking or running.

\begin{figure}
	\centering
	\includegraphics[width=10cm]{Aufbauohnelizenz.png}
	\caption{Schematics of the arrangement for the recording of our source data.}
	\label{fig:schematic}
\end{figure}

\subsection{Recording Subjects}

Once everything was in place and we were relatively comfortable with our software and the process of recording, we asked random acquaintances to be subjects in our project.
Each subject had to do either or both of the motions with and without the weights on them.
We thus recorded 19 data sets of the jumping jack and 16 data sets of the rope skipping.
Each data set consists of one recording with the weight and one without.
The order in which the subjects either started or finished with the weight was equally divided among the group to minimize effects of fatigue.

As the Kinect senses the 3D spatial information via infrared, we initially had trouble when test subjects wore dark clothing.
This was due to the dark clothing absorbing the infrared, thus becoming invisible to the Kinect.
To solve this problem, we had all subjects that wore dark clothing wear a blue overall over their clothes to assure a steady recording.

For the actual recording, we first explained the procedure to our subjects.
This was done so that the motions would be the same between all subjects.
We then had the subjects start with the motion and then pressed record.
For the recording of jumping jacks, our software automatically recognized the start position and counted 10 cycles of jumping jacks.
These 10 cycles were then saved in a text file consisting of the 3D vectors of the 16 skeleton points over the frames in time.
% The recording of the skipping rope proved to be more difficult, so we eyeballed 10 successful cycles for our recordings.

An example of the recording can be seen here: \url{http://youtu.be/CJLkOyAGtjk}. The Kinect stands to the right of the camera, at the same relative distance.

\section{Creating Motionvectors to Represent Motion Data}

Having saved all of the skeleton data received from the Kinect into textfields the representation of the jumping motion of every subject is a huge matrix. Each row of this matrix represents one single posture of the subject's body, which consists of 48 values:
Each of the 16 joints is represented by three floating-point numbers specifying the three-dimensional position of the joint.
The postures can also be seen as points in the 48 dimensional space.

This data is highly redundant.
For example, jumping jack and jump rope are symmetrical motions.
The right arm moves the same way as the left arm does except for reflection.
The same goes for the legs.

Therefore we hoped that the dimensionality of the postures could be drastically reduced without losing much of the data's information using principal component analysis (PCA).
Basically, what PCA does is to rotate and move the 48 dimensional cartesian coordinate system, in which every possible three-dimensional 16-joint-skeleton posture can be represented as one point, so that for all n $\in \left\{1, ..., 48\right\}$ the first n axes account for as much as possible of the variance of the postures of one subject's jumping motion.
The vectors that represent the axes of the in this way transformed coordinate system are called the principal components.
The origin of this coordinate system is the average of all postures, which we are going to refer to as average posture $p_{0}$.
The principal components are the eigenvectors of the covariance matrix of the skeleton data set.
Like in \cite{origin} we are going to refer to these eigenvectors as "eigenpostures" to distinguish them from the eigenvectors of another PCA we are going to use later.

% \begin{Andi}
\subsection{Sinusoidal--Approximation of Coefficients}
A graphical representation of an \emph{eigenposture} for a jumping jack motion can be seen on top of figure \ref{fig:approx}.
Here the blue curve represents the first coefficient of the \emph{eigenposture} and the green curve the second coefficient.
Apparently this graph looks similar to a sinusoidal graph.
The reason for this characteristic is that our jumping motions are cyclic motions.
Therefore the coefficients of our \emph{eigenpostures} can be approximated using sinusoidal functions.
This is done similar to the approach of \cite{origin}.
Hereby a sine function is used to describe each of the coefficients of the \emph{eigenposture}.
The approximation is done using MATLABs integrated \emph{fit} function.
The fitted graph of the example mentioned above can be seen at the bottom of figure \ref{fig:approx}.

\begin{figure}[htb]
		\centering
		%\includegraphics[width=\textwidth]{blubblubblub.png}
		\caption{Original dataset (top) and sinusoidal approximation (bottom).}
		\label{fig:approx}
\end{figure}

MATLABs \emph{fit} function also outputs the goodness of the fit in form of the square of the correlation ()\emph{rsquare}).
Here values close the zero mean a bad fit and values close to one a good fit.
A value of one would be an exact fit with no information lost.
The goodness of the fit shown in figure \ref{fig:approx} is 0.90225 for the first and 0.80946 for the second coefficient.
This is a rather good result when considered, that we use a motion of ten full cycles as input and every difference in each of the cycles impairs the approximation. 
Also the approximation goodness of this example is close the the average goodness of all our jumping jack \emph{eigenpostures}.
This also counts for the following examples.

Also when looking back at our jumping jack example in figure \ref{fig:approx} we can see that especially the first coefficient is rather badly approximated on the top of the wave.
We tried therefore an additional approach to get even better results. That is to use two overlapping sine functions for a better representation of each coefficient.
The result of this approach can be seen in figure \ref{fig:approx2}.
Here the \emph{rsquare} values are 0.98515 for the first and 0.9366 for the second coefficient and thus represents a better model of our jumping jack motion.

\begin{figure}[htb]
		\centering
		%\includegraphics[width=\textwidth]{blubblubblub.png}
		\caption{Original dataset (top) and sinusoidal approximation using two overlapping sine functions (bottom).}
		\label{fig:approx2}
\end{figure}

The skipping motion, as seen in figure \ref{fig:approx3}, however does not have this characteristic so here only one sine function was used for our further work.
The \emph{rsquare} values here are 0.98886 and 0.84932 for the two coefficients.

\begin{figure}[htb]
		\centering
		%\includegraphics[width=\textwidth]{blubblubblub.png}
		\caption{Original dataset of a skipping motion (top) and its sinusoidal approximation (bottom).}
		\label{fig:approx3}
\end{figure}


\subsection{Comparison of original and after-processing-motions}

% TODO Animationen vergleichen / in pdf einbinden?

\subsection{Creation of Motion Vectors}

% TODO Aufbau der Bewegunsvectoren

% TODO Testpersonen, wohin?


% \end{Andi}
\section{Classification}

For now each jumping motion is represented by a 141/143/144/146-dimensional motionvector. Earlier we used PCA to reduce the dimensionality of all 48-dimensional representations of skeleton postures of one single jumping motion. Now we are going to do the same with all the 141/143/144/146-dimensional representations of the complete jumping motions. However this time the unsimilarity of the data in the motionvectors might affect how good the PCA will work. The first PCA's input were 48-dimensional postures, each of the 48 entries was a distance describing one dimension of the three-dimensional position of a joint. All of the entries in each posture were comparable. The entries in the motionvector, however, described positions, frequencies, amplitudes and phases, thus the values of one motionvector had various magnitudes. Therefore we created a 141/143/144/146-dimensional standard deviation vector u containing the standard deviations of each motionvector-entry of the complete set of motionvectors. Each entry of each motionvector was then divided by the corresponding entry of u prior to running the PCA. The vector u was saved to file for later use.

The result of this second PCA is another set of eigenvectors, which we are going to refer to as the eigenjumpers comparable to the eigenwalkers of \cite{origin}, and for each jumping motion a set of coefficients. Given the eigenjumpers and the average motionvector one can reconstruct any motionvector by using the coefficients that belong to the motionvector. Exactly as with the first PCA this is achieved by adding up the average motionvector with the products of each eigenjumper with the corresponding coefficient.

\section{Synthesizing of Jumping Motions}

\section{Visualisation}

This section is independent from the rest of the paper as the analytical methods don't matter. Our intent is to show the captured points of the skeleton in an representative 3D Model.

\subsection{Absolute vs. Relative Values}
As seen in the description of storing points in a ".txt" file the kinect uses Absolute Values to represent the skeleton  - which is absolutely useful, because otherwise it couldn't detect different lengths in body parts. Instead it would had to map the measured values on a defined body model. This procedure would cause inaccuracy.\
While animating a defined model in the opposite you can't just easily scale single body parts to adjust the proportions. This problem needs as a solution a format invariant to scaling.

\subsection{The .bvh file}
The software used for testing the following part is Poser 9 from SmithMicro.\footnote{http://www.software3d.de/figurendesign/smithmicro-poser-9.html}
A .bvh file is made up of two parts: The bodystructure and the frames.\
The bodystructure defines the relationship between each bodypart. As one might see in
Listing 1 this structure is build hierarchical. Each joint is represented by a name and the connection to its successors. Changing the value in a joint affects all the successors. 
The standardpose (all angles in between 0) is the so called T-Pose. You can see the model standing with parallel feet looking straight forward and the arms are bend 90 degrees relative to the body.
\begin{lstlisting}[caption=.bvh Connected Structure]{Name}
JOINT lCollar
{
		OFFSET	1.858774 10.237258 -1.915852
		CHANNELS 3 Zrotation Yrotation Xrotation
		JOINT lShldr
		{
			OFFSET	6.103279 0.000102 -0.038601
			CHANNELS 3 Zrotation Yrotation Xrotation
			JOINT lForeArm
\end{lstlisting}

Responseable for the change over time is the second part of the file. In here we define the joint-angles. Really useful for this aspect of the project was the similarity between the recorded ".txt" file and the seond part of the ".bvh". Therefore we could calculate the absolute positions in a row from the .txt-File to angles and replace the current lines in the .bvh by the ones generated by a matlab script which will be shown in the following section.
\begin{lstlisting}[caption=Motionpart of .bvh]{Name}
MOTION
Frames:     160
Frame Time: 1                
0 50.946 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -30.66 0 0 -30.295 0 0 10.811 ...
0 51.229 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -29.93 0 0 -30.695 0 0 9.6152 ...
\end{lstlisting}

\subsection{Calculating the angles}
After finding the corresponding entries in the ".txt" and ".bvh" file now is the time to convert values. Using Matlab as  intermediary between the two formats we take the advantage of easy handling .txt to matrices. 
\newline
The problem is now solved by the vector based equation:
\newline
\begin{math}
\vec a \cdot \vec b = \left|\vec a \right| \left|\vec b \right| \cos \phi
\end{math}
\begin{lstlisting}[caption=Calculating the angle]{Name}
D = dlmread('nameSeilhuepfen0.txt');
[...]

# Rechter Oberarm Eintrag Nr.25 (Vektor 24x Daten sind schon vorhanden)
m46 = (D(ind,17)-D(ind,11))/(D(ind,16)-D(ind,10));

# Vektorenberechnung. Nummerierung erfolgt von Punktnummer 
# aus Kinectmodells
Vektor46y = D(ind,17)-D(ind,11);
Vektor46x = D(ind,16)-D(ind,10);
skalarprodukt=Vektor24y*Vektor46y+Vektor24x*Vektor46x;
laengeeins=sqrt(Vektor24y^2 + Vektor24x^2);
laengezwei=sqrt(Vektor46y^2 + Vektor46x^2); 
wOberarm_rechts = radtodeg(acos(skalarprodukt/(laengeeins*laengezwei)));
\end{lstlisting}


\begin{figure}
	\centering
	\includegraphics[width=8cm]{3dRender.png}
	\caption{Result of rendering.}
	\label{fig:programm}
\end{figure}


\section{Conclusion}

TODO: Put conclusion here.

\newpage
\bibliography{doc}

\end{document}